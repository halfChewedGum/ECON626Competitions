{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halfChewedGum/ECON626Competitions/blob/main/assignment2(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e563d718-12e8-4c4b-a63f-2bf6827d45a0",
      "metadata": {
        "id": "e563d718-12e8-4c4b-a63f-2bf6827d45a0"
      },
      "source": [
        "- library to use: sklearn\n",
        "\n",
        "Task 1. Multinomial Naive Bayes, \n",
        "\n",
        "class positive, negative\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da0fa4e5-e007-49bd-a234-fd1051b1d60e",
      "metadata": {
        "id": "da0fa4e5-e007-49bd-a234-fd1051b1d60e"
      },
      "source": [
        "Step 0. Pre-processing\n",
        "\n",
        "1. Read data\n",
        "2. Tokenize\n",
        "3. Bag of Words \n",
        "4. Train, Development, Test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb1095bf-946d-4b3f-ab54-41d2d730686f",
      "metadata": {
        "id": "bb1095bf-946d-4b3f-ab54-41d2d730686f"
      },
      "outputs": [],
      "source": [
        "#open file \n",
        "path_to_pos = ''\n",
        "path_to_neg = ''\n",
        "\n",
        "pos_text = []\n",
        "with open('pos.txt') as f:\n",
        "    pos_text = f.readlines()\n",
        "    posText = f.read()\n",
        "    \n",
        "#pos_text is now a list of lines from the text file. \n",
        "#posText is the entire text file. \n",
        "    \n",
        "f.close()    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKRMOUCckxql",
        "outputId": "ddc76890-460e-4965-f5df-4570acdb9aef"
      },
      "id": "gKRMOUCckxql",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fe0ed798-e29f-4f3f-896a-d0c186bc0361",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "fe0ed798-e29f-4f3f-896a-d0c186bc0361",
        "outputId": "b5eadce4-d8b1-4f86-9b77-b2264a33ce8d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5e6c2faa68b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpositives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pos.txt'"
          ]
        }
      ],
      "source": [
        "with open('pos.txt') as file1:\n",
        "    positives = file1.read()\n",
        "\n",
        "print(type(positives))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "cmUVhCtHkMzZ"
      },
      "id": "cmUVhCtHkMzZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7c1e17-c447-4462-804e-ad5b1a6427a2",
      "metadata": {
        "id": "5e7c1e17-c447-4462-804e-ad5b1a6427a2"
      },
      "outputs": [],
      "source": [
        "with open('neg.txt') as file2:\n",
        "    negatives = file2.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74490a46-12a8-4d50-941f-580b9c1ae76b",
      "metadata": {
        "id": "74490a46-12a8-4d50-941f-580b9c1ae76b",
        "outputId": "7c0961d6-220a-43f6-f0d2-a93dd665a761"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My daughter wanted this book and the price on Amazon was the best.\n",
            "She has already tried one recipe a day after receiving the book.\n",
            "I bought this zoku quick pop for my daughterr with her zoku quick maker.\n",
            "She loves it and have fun to make her own ice cream.\n",
            "I was hoping there were more where those came from.\n",
            "This book emphasizes very sweet dessert pops, however.\n",
            "There are 41 recipes in total, only 13 of which are fruit pops.\n",
            "There is a \"Fresh and Fruity\" chapter, followed by three chapters of dessert pops entitled \"I Scream for Quick Pops!\", \"Bake Shop\", and  \"Coco Loco\".\n",
            "As you might guess from the last one, there are 15 pop recipes that contain chocolate.Chapters on \"Tips\" and \"Techniques\" are useful.\n",
            "There is more detailed information about ingredients that don't freeze well in the Zoku than is found in the instruction manual.\n",
            "The pages about core pops are especially helpful, as they include recipes for pink, purple, and orange outer layers for brightly-colored core pops.\n",
            "Many, incl\n"
          ]
        }
      ],
      "source": [
        "print(positives[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6b5609-eca2-46b6-9810-d163830e250a",
      "metadata": {
        "id": "fe6b5609-eca2-46b6-9810-d163830e250a",
        "outputId": "f106b65d-8376-42d0-9dbd-9f4502ffb4f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive reviews words length:  32181338\n",
            "negative reviews words length:  32432428\n"
          ]
        }
      ],
      "source": [
        "print('positive reviews words length: ', len(positives))\n",
        "print('negative reviews words length: ', len(negatives))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c47a61fe-d303-4a97-9b42-209793e04118",
      "metadata": {
        "id": "c47a61fe-d303-4a97-9b42-209793e04118"
      },
      "source": [
        "- Dataset of all the words that appear in the \"positive\" reviews. (similar DS for negative)\n",
        "\n",
        "|word|frequency|label|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025ae3fa-eea0-41ff-9275-2e81ec74af65",
      "metadata": {
        "id": "025ae3fa-eea0-41ff-9275-2e81ec74af65"
      },
      "outputs": [],
      "source": [
        "#remove punctuations and special characters \n",
        "import string \n",
        "import re \n",
        "\n",
        "#all lower case: \n",
        "\n",
        "positives = positives.lower()\n",
        "negatives = negatives.lower()\n",
        "\n",
        "#remove all punctuations \n",
        "# string.punctuation \n",
        "positive_nopun = \"\".join([i for i in positives if i not in string.punctuation])\n",
        "negative_nopun = \"\".join([i for i in negatives if i not in string.punctuation])\n",
        "\n",
        "#remove all special characters \n",
        "positive_reviews = re.sub('[^A-Za-z0-9]+ ', '', positive_nopun)\n",
        "negative_reviews = re.sub('[^A-Za-z0-9]+ ', '', negative_nopun)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d411689b-c973-47be-bbfe-7fe7b64d2516",
      "metadata": {
        "id": "d411689b-c973-47be-bbfe-7fe7b64d2516",
        "outputId": "bb45aade-5d63-46b5-de7a-82f019ef287d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type of new data:  <class 'str'>\n",
            "size of new data - positives:  31198552\n",
            "size of new data - negatives:  31405163\n",
            "positive reviews first 1000 characters:  my daughter wanted this book and the price on amazon was the best\n",
            "she has already tried one recipe a day after receiving the book\n",
            "i bought this zoku quick pop for my daughterr with her zoku quick maker\n",
            "she loves it and have fun to make her own ice cream\n",
            "i was hoping there were more where those came from\n",
            "this book emphasizes very sweet dessert pops however\n",
            "there are 41 recipes in total only 13 of which are fruit pops\n",
            "there is a fresh and fruity chapter followed by three chapters of dessert pops entitled i scream for quick pops bake shop andcoco loco\n",
            "as you might guess from the last one there are 15 pop recipes that contain chocolatechapters on tips and techniques are useful\n",
            "there is more detailed information about ingredients that dont freeze well in the zoku than is found in the instruction manual\n",
            "the pages about core pops are especially helpful as they include recipes for pink purple and orange outer layers for brightlycolored core pops\n",
            "many including the recipe for a vanilla base cal\n"
          ]
        }
      ],
      "source": [
        "print('type of new data: ', type(positive_reviews))\n",
        "print('size of new data - positives: ', len(positive_reviews))\n",
        "print('size of new data - negatives: ', len(negative_reviews))\n",
        "print('positive reviews first 1000 characters: ', positive_reviews[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8181a565-564e-44a4-b6f0-87d504203189",
      "metadata": {
        "id": "8181a565-564e-44a4-b6f0-87d504203189",
        "outputId": "0152e6d8-f7d3-4118-fa95-870f9acccf6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "how many characters/words got deleted in positive?  982786\n",
            "how many characters/words got deleted in negative?  1027265\n"
          ]
        }
      ],
      "source": [
        "words_in_pos_before = 32181338\n",
        "words_in_neg_before = 32432428\n",
        "words_in_pos_after = 31198552\n",
        "words_in_neg_after = 31405163\n",
        "diff_positives = words_in_pos_before - words_in_pos_after\n",
        "diff_negatives = words_in_neg_before - words_in_neg_after\n",
        "\n",
        "print('how many characters/words got deleted in positive? ', diff_positives)\n",
        "print('how many characters/words got deleted in negative? ', diff_negatives)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6aa86c3-c9e7-471c-b81b-9bb17e6bdc40",
      "metadata": {
        "id": "d6aa86c3-c9e7-471c-b81b-9bb17e6bdc40",
        "outputId": "c1a87ed8-707e-4926-d9e5-221b1636baf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "#tokenize or split \n",
        "\n",
        "positive_words = positive_reviews.split()\n",
        "negative_words = negative_reviews.split()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad40e8a-6c16-4cac-aff3-f79bd59de373",
      "metadata": {
        "id": "5ad40e8a-6c16-4cac-aff3-f79bd59de373",
        "outputId": "d8c0fdea-eae4-4b79-b6e4-15f32095fc07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting more-itertools\n",
            "  Downloading more_itertools-8.13.0-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 491 kB/s eta 0:00:01\n",
            "\u001b[?25hInstalling collected packages: more-itertools\n",
            "Successfully installed more-itertools-8.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install more-itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f133d1b-15a9-4577-bf9e-4d36786815aa",
      "metadata": {
        "id": "2f133d1b-15a9-4577-bf9e-4d36786815aa",
        "outputId": "97dfe544-f4dc-4b24-9597-c540ecd42e77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "[('my', 69563), ('daughter', 1017), ('wanted', 3373), ('this', 97282), ('book', 543), ('and', 183268), ('the', 297529), ('price', 9921), ('on', 48532), ('amazon', 4107)]\n"
          ]
        }
      ],
      "source": [
        "#time to fill up the datasets. let us now count how many of each word are in each list \n",
        "from collections import Counter\n",
        "import more_itertools\n",
        "\n",
        "counter_positive_words = dict(Counter(positive_words))\n",
        "counter_negative_words = dict(Counter(negative_words))\n",
        "\n",
        "print(type(counter_positive_words))\n",
        "print(more_itertools.take(10, counter_positive_words.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "324ca25d-cbaa-4b91-b3bb-b24b3cfdae85",
      "metadata": {
        "id": "324ca25d-cbaa-4b91-b3bb-b24b3cfdae85",
        "outputId": "196e1fdc-bd29-4939-c9f1-9188d0db1893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'bought', 'this', 'when', 'the', 'pop', 'maker', 'as', 'for', 'embellishing']\n",
            "[69563, 1017, 3373, 97282, 543]\n"
          ]
        }
      ],
      "source": [
        "#add this information to dictionaries: \n",
        "\n",
        "list_pos_words = []\n",
        "list_neg_words = []\n",
        "list_pos_freqs = []\n",
        "list_neg_freqs = []\n",
        "\n",
        "\n",
        "for key, value in counter_positive_words.items():\n",
        "    list_pos_words.append(key)\n",
        "    list_pos_freqs.append(value)\n",
        "    \n",
        "\n",
        "for key, value in counter_negative_words.items():\n",
        "    list_neg_words.append(key)\n",
        "    list_neg_freqs.append(value)\n",
        "    \n",
        "# print(list_neg_words[:10])\n",
        "# print(list_pos_freqs[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7d0384-a80f-4136-a71d-05e48c369c0c",
      "metadata": {
        "id": "7f7d0384-a80f-4136-a71d-05e48c369c0c"
      },
      "outputs": [],
      "source": [
        "#put all info in dictionaries to later convert to dataframes \n",
        "positive_dict_info = {\n",
        "    'word': list_pos_words,\n",
        "    'frequency': list_pos_freqs,\n",
        "    'label': ['positive'] * len(list_pos_words)\n",
        "}\n",
        "\n",
        "negative_dict_info = {\n",
        "    'word': list_neg_words,\n",
        "    'frequency': list_neg_freqs,\n",
        "    'label': ['negative'] * len(list_neg_words)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388723a8-e0db-44e0-8037-3bb5be308257",
      "metadata": {
        "id": "388723a8-e0db-44e0-8037-3bb5be308257",
        "outputId": "ba907cb1-c50c-435a-be7c-d39b384193dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       word  frequency     label\n",
            "0        my      69563  positive\n",
            "1  daughter       1017  positive\n",
            "2    wanted       3373  positive\n",
            "3      this      97282  positive\n",
            "4      book        543  positive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd \n",
        "\n",
        "positive_db = pd.DataFrame.from_dict(positive_dict_info)\n",
        "print(positive_db.head())\n",
        "\n",
        "\n",
        "negative_db = pd.DataFrame.from_dict(negative_dict_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d69c4f-2e29-473c-a6c7-90e7c109864b",
      "metadata": {
        "id": "c5d69c4f-2e29-473c-a6c7-90e7c109864b"
      },
      "outputs": [],
      "source": [
        "#just for peace of mind, save copies on disk \n",
        "\n",
        "positive_db.to_csv('positive_set.csv')\n",
        "negative_db.to_csv('negative_set.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3918d4-8b18-4bf9-868b-e60fa0ed5d40",
      "metadata": {
        "id": "3d3918d4-8b18-4bf9-868b-e60fa0ed5d40",
        "outputId": "6149a278-fb15-48c4-fe4b-439f7f14e433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       word  frequency     label\n",
            "0        my      69563  positive\n",
            "1  daughter       1017  positive\n",
            "2    wanted       3373  positive\n",
            "3      this      97282  positive\n",
            "4      book        543  positive\n",
            "                      word  frequency     label\n",
            "127188             nfusion          1  negative\n",
            "127189             bugeyed          1  negative\n",
            "127190            simmilar          1  negative\n",
            "127191     wellinteresting          1  negative\n",
            "127192  buttonreactiontime          1  negative\n"
          ]
        }
      ],
      "source": [
        "#merget both datasets into one big database\n",
        "\n",
        "df = pd.concat([positive_db, negative_db]) \n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea8caa6a-c707-4dee-afff-2674b4b40971",
      "metadata": {
        "id": "ea8caa6a-c707-4dee-afff-2674b4b40971"
      },
      "source": [
        "If any two words are repeated in both, then choose the one with higher frequency and get rid of the other "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf229b4-dd01-4055-9ffa-3051c6155072",
      "metadata": {
        "id": "fdf229b4-dd01-4055-9ffa-3051c6155072"
      },
      "outputs": [],
      "source": [
        "p_w_l = list(positive_db['word'])\n",
        "n_w_l = list(negative_db['word'])\n",
        "\n",
        "count = 0\n",
        "for i in range(len(p_w_l)):\n",
        "    for j in range(len(n_w_l)):\n",
        "        if p_w_l[i] == n_w_l[j]:\n",
        "            count += 1\n",
        "        else:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8687e093-5d2f-40a6-a669-4b2a35e34931",
      "metadata": {
        "id": "8687e093-5d2f-40a6-a669-4b2a35e34931",
        "outputId": "92144152-2558-4717-ba28-eb98dfd961d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "this many words are repeated in both corpus:  37692\n"
          ]
        }
      ],
      "source": [
        "print('this many words are repeated in both corpus: ', count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8b6108-d69c-41ed-83de-d526b8c0dfe0",
      "metadata": {
        "id": "ac8b6108-d69c-41ed-83de-d526b8c0dfe0"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(positive_db, negative_db, on = ['word'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe3a0631-a4ea-44c8-a184-0d43a6855867",
      "metadata": {
        "id": "fe3a0631-a4ea-44c8-a184-0d43a6855867",
        "outputId": "214c83c3-3c94-4456-c990-fccb1c33098d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>frequency_x</th>\n",
              "      <th>label_x</th>\n",
              "      <th>frequency_y</th>\n",
              "      <th>label_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>my</td>\n",
              "      <td>69563</td>\n",
              "      <td>positive</td>\n",
              "      <td>62280</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>daughter</td>\n",
              "      <td>1017</td>\n",
              "      <td>positive</td>\n",
              "      <td>1344</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wanted</td>\n",
              "      <td>3373</td>\n",
              "      <td>positive</td>\n",
              "      <td>3067</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this</td>\n",
              "      <td>97282</td>\n",
              "      <td>positive</td>\n",
              "      <td>100098</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>book</td>\n",
              "      <td>543</td>\n",
              "      <td>positive</td>\n",
              "      <td>372</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       word  frequency_x   label_x  frequency_y   label_y\n",
              "0        my        69563  positive        62280  negative\n",
              "1  daughter         1017  positive         1344  negative\n",
              "2    wanted         3373  positive         3067  negative\n",
              "3      this        97282  positive       100098  negative\n",
              "4      book          543  positive          372  negative"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43180bd8-46b5-4e9f-b9f5-9d1e7e9024f7",
      "metadata": {
        "id": "43180bd8-46b5-4e9f-b9f5-9d1e7e9024f7",
        "outputId": "1a6b7e0d-f53d-4e89-85b2-9808adc90944"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>frequency_x</th>\n",
              "      <th>label_x</th>\n",
              "      <th>frequency_y</th>\n",
              "      <th>label_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37687</th>\n",
              "      <td>pf</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>7</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37688</th>\n",
              "      <td>swimmer</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37689</th>\n",
              "      <td>fatality</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37690</th>\n",
              "      <td>fouth</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37691</th>\n",
              "      <td>yat</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           word  frequency_x   label_x  frequency_y   label_y\n",
              "37687        pf            1  positive            7  negative\n",
              "37688   swimmer            1  positive            1  negative\n",
              "37689  fatality            1  positive            2  negative\n",
              "37690     fouth            1  positive            1  negative\n",
              "37691       yat            1  positive            1  negative"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b697409-c45a-472e-82d2-fd6d395d2fb7",
      "metadata": {
        "id": "0b697409-c45a-472e-82d2-fd6d395d2fb7"
      },
      "source": [
        "Then, for all the ones with different values of label - if the frequency_l > frequency_r then label_l is chosen and added to new column 'label'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f51b564-0085-455e-ba0b-d5203df44e84",
      "metadata": {
        "id": "4f51b564-0085-455e-ba0b-d5203df44e84",
        "outputId": "b3da2c94-d3d1-4762-cb42-0821a029f5aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>frequency_x</th>\n",
              "      <th>label_x</th>\n",
              "      <th>frequency_y</th>\n",
              "      <th>label_y</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>my</td>\n",
              "      <td>69563</td>\n",
              "      <td>positive</td>\n",
              "      <td>62280</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>daughter</td>\n",
              "      <td>1017</td>\n",
              "      <td>positive</td>\n",
              "      <td>1344</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wanted</td>\n",
              "      <td>3373</td>\n",
              "      <td>positive</td>\n",
              "      <td>3067</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this</td>\n",
              "      <td>97282</td>\n",
              "      <td>positive</td>\n",
              "      <td>100098</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>book</td>\n",
              "      <td>543</td>\n",
              "      <td>positive</td>\n",
              "      <td>372</td>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       word  frequency_x   label_x  frequency_y   label_y     label\n",
              "0        my        69563  positive        62280  negative  positive\n",
              "1  daughter         1017  positive         1344  negative  negative\n",
              "2    wanted         3373  positive         3067  negative  positive\n",
              "3      this        97282  positive       100098  negative  negative\n",
              "4      book          543  positive          372  negative  positive"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df['label'] = merged_df['label_x'].where(merged_df['frequency_x'] > merged_df['frequency_y'], merged_df['label_y'])\n",
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9561668a-d044-4447-b632-9f15ff45bba9",
      "metadata": {
        "id": "9561668a-d044-4447-b632-9f15ff45bba9",
        "outputId": "fa773dfc-914e-4537-b2ca-1df7eca3c33d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>frequency_x</th>\n",
              "      <th>label_x</th>\n",
              "      <th>frequency_y</th>\n",
              "      <th>label_y</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37687</th>\n",
              "      <td>pf</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>7</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37688</th>\n",
              "      <td>swimmer</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37689</th>\n",
              "      <td>fatality</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37690</th>\n",
              "      <td>fouth</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37691</th>\n",
              "      <td>yat</td>\n",
              "      <td>1</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           word  frequency_x   label_x  frequency_y   label_y     label\n",
              "37687        pf            1  positive            7  negative  negative\n",
              "37688   swimmer            1  positive            1  negative  negative\n",
              "37689  fatality            1  positive            2  negative  negative\n",
              "37690     fouth            1  positive            1  negative  negative\n",
              "37691       yat            1  positive            1  negative  negative"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7ac0d5d-282f-4f60-9e04-4b2dadf70e36",
      "metadata": {
        "id": "a7ac0d5d-282f-4f60-9e04-4b2dadf70e36",
        "outputId": "b3d4d9de-058f-4e87-dff5-a36d8494353e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>my</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>daughter</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wanted</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>book</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37687</th>\n",
              "      <td>pf</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37688</th>\n",
              "      <td>swimmer</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37689</th>\n",
              "      <td>fatality</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37690</th>\n",
              "      <td>fouth</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37691</th>\n",
              "      <td>yat</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>37692 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           word     label\n",
              "0            my  positive\n",
              "1      daughter  negative\n",
              "2        wanted  positive\n",
              "3          this  negative\n",
              "4          book  positive\n",
              "...         ...       ...\n",
              "37687        pf  negative\n",
              "37688   swimmer  negative\n",
              "37689  fatality  negative\n",
              "37690     fouth  negative\n",
              "37691       yat  negative\n",
              "\n",
              "[37692 rows x 2 columns]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "merged_df.drop(columns = ['frequency_x', 'frequency_y', 'label_x', 'label_y'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11165994-56d9-45ae-9f2b-0d86406dd790",
      "metadata": {
        "id": "11165994-56d9-45ae-9f2b-0d86406dd790"
      },
      "outputs": [],
      "source": [
        "similar_words = list(merged_df['word'])\n",
        "\n",
        "for i in range(len(similar_words)):\n",
        "    for j in range(len(list(df['word']))):\n",
        "        if df.iloc[j,1] == similar_words[i]:\n",
        "            df.drop(i, axis = 0, inplace = True)\n",
        "\n",
        "        \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0e8463-10af-4efb-820a-d4100cff3a1e",
      "metadata": {
        "id": "1d0e8463-10af-4efb-820a-d4100cff3a1e"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.drop(columns = ['frequency'])\n",
        "database_full = pd.concat(merged_df, df)\n",
        "\n",
        "print(database_full.head())\n",
        "print('----------------------------------')\n",
        "print(database_full.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04eeef25-7c9a-450b-b087-0dd0df9953a8",
      "metadata": {
        "id": "04eeef25-7c9a-450b-b087-0dd0df9953a8"
      },
      "source": [
        "As per instructions, Training data is 80%, test data is 10% and development/validation data is 10%. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a4ad69-3068-446d-861a-90f50db28093",
      "metadata": {
        "id": "80a4ad69-3068-446d-861a-90f50db28093"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = database_full['label']\n",
        "x = database_full['word']\n",
        "\n",
        "x_train, x_part2, y_train, y_part2 = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "x_test, x_dev, y_test, y_dev = train_test_split(x_part2, y_part2, test_size = 0.5, random_state = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6850730f-57d4-4c48-8142-7201ece090f8",
      "metadata": {
        "id": "6850730f-57d4-4c48-8142-7201ece090f8"
      },
      "outputs": [],
      "source": [
        "#save all the files into excel csv files\n",
        "\n",
        "x_train.to_csv('x_train.csv')\n",
        "y_train.to_csv('y_train.csv')\n",
        "x_test.to_csv('x_test.csv')\n",
        "y_test.to_csv('y_test.csv')\n",
        "x_dev.to_csv('x_dev.csv')\n",
        "y_dev.to_csv('y_dev.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f0edfe-b28f-4092-8005-f7bc3565ae79",
      "metadata": {
        "id": "05f0edfe-b28f-4092-8005-f7bc3565ae79"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "015281e4-33ce-4bbb-aaf0-dd26c44b90ba",
      "metadata": {
        "id": "015281e4-33ce-4bbb-aaf0-dd26c44b90ba"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7d80bf0-c08b-4485-9b47-1d027bf10c67",
      "metadata": {
        "id": "d7d80bf0-c08b-4485-9b47-1d027bf10c67"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1514585b-8eb2-48a7-b72f-0227e5f9e24f",
      "metadata": {
        "id": "1514585b-8eb2-48a7-b72f-0227e5f9e24f"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb98774a-0815-4c89-bc29-c76fcffb2518",
      "metadata": {
        "id": "bb98774a-0815-4c89-bc29-c76fcffb2518"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "import pickle \n",
        "\n",
        "\n",
        "def remove_stopwords(dat):\n",
        "    v = CountVectorizer(stop_words='english')\n",
        "    dat_ = v.fit_transform(dat).toarray()\n",
        "    \n",
        "    return dat_\n",
        "\n",
        "def train_naive_bayes(xd, yd, xdt, ydt):\n",
        "    model = MultinomialNB()\n",
        "    model.fit(xd, yd)\n",
        "    score = model.score(xdt, ydt)\n",
        "    prediction = model.predict(xdt)\n",
        "    \n",
        "    return model, score, prediction\n",
        "    \n",
        "def naive_bayes_results(y_true, y_pred):\n",
        "    results = list(precision_recall_fscore_support(y_true, y_pred, average = 'weighted'))\n",
        "    precision = results[0]\n",
        "    recall = results[1]\n",
        "    fscore = results[2]\n",
        "    \n",
        "    print('Results of Multinomial Naive Bayes: ')\n",
        "    print('------------------------------------')\n",
        "    print('precision : ', precision)\n",
        "    print('recall : ', recall)\n",
        "    print('fscore : ', fscore)\n",
        "    \n",
        "def unigram(exl):\n",
        "    \"\"\"\n",
        "    Takes as input a list ( [1,2,3,4] ) \n",
        "    returns single elements one-by-one in order \n",
        "    [(1),(2),(3),(4)]\n",
        "    \"\"\"\n",
        "    emp = []\n",
        "    for i in range(len(exl)):\n",
        "        emp.insert(i, tuple(exl[i]))\n",
        "    \n",
        "    return emp\n",
        "\n",
        "def bigram(exl):\n",
        "    \"\"\"\n",
        "    Takes as input a list ( [1,2,3,4] )\n",
        "    returns pairs of elements two-by-two in order:\n",
        "    [(1,2), (2,3), (3,4)]\n",
        "    \"\"\"\n",
        "    emp = []\n",
        "    \n",
        "    for i in range(len(exl)-1):\n",
        "        emp.insert(i, (exl[i],exl[i+1]))\n",
        "    \n",
        "    return emp \n",
        "\n",
        "def uni_bigram(exl):\n",
        "    \"\"\"\n",
        "    Takes as input a list ( [1,2,3,4] )\n",
        "    returns pairs of elements two-by-two in order:\n",
        "    [(1),(2),(3),(4),(1,2), (2,3), (3,4)]\n",
        "    \"\"\"\n",
        "    \n",
        "    uni = unigram(exl)\n",
        "    bi = bigram(exl)\n",
        "    uni_bi = uni.extend(bi)\n",
        "    \n",
        "    return uni_bi\n",
        "\n",
        "\n",
        "def call_for_results(xd, xdt, yd, ydt):\n",
        "    s = train_naive_bayes(xd, xdt, yd, ydt)[1]\n",
        "    p = train_naive_bayes(xd, xdt, yd, ydt)[2]\n",
        "    \n",
        "    print('Score for NB(1.1): ', s)\n",
        "    print('-----------------')\n",
        "    \n",
        "    naive_bayes_results(ydt, p)\n",
        "\n",
        "    return \"---------------------\"\n",
        "\n",
        "def make_pickl_file(fn, m):\n",
        "    pickle.dump(m, open(fn, 'wb')\n",
        "                \n",
        "    return 'pickled.'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec1c2013-72ee-4917-aecf-87b5b0983ad4",
      "metadata": {
        "id": "ec1c2013-72ee-4917-aecf-87b5b0983ad4"
      },
      "source": [
        "1.1 Stopwords Removed, Unigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9c8dd1-c7e9-4acf-a8ae-1b8680cd0b51",
      "metadata": {
        "id": "dc9c8dd1-c7e9-4acf-a8ae-1b8680cd0b51"
      },
      "outputs": [],
      "source": [
        "#remove stopwords \n",
        "x_train_1 = remove_stopwords(x_train) \n",
        "x_dev_1 = remove_stopwords(x_dev)\n",
        "x_test_1 = remove_stopwords(x_test)\n",
        "y_train_1 = remove_stopwords(y_train)\n",
        "y_dev_1 = remove_stopwords(y_dev)\n",
        "y_test_1 = remove_stopwords(y_test)\n",
        "\n",
        "#unigrams \n",
        "x_train_uni_11 = unigram(x_train_1)\n",
        "x_dev_uni_11 = unigram(x_dev_1)\n",
        "x_test_uni_11 = unigram(x_test_1)\n",
        "y_train_uni_11 = unigram(y_train_1)\n",
        "y_dev_uni_11 = unigram(y_dev_1)\n",
        "y_test_uni_11 = unigram(y_test_1)\n",
        "\n",
        "\n",
        "#parameter tuning for later! \n",
        "call_for_results(x_train_1, x_test_1,y_train_1, y_test_1)\n",
        "\n",
        "#pickling\n",
        "file_11 = 'mnb_uni_ns.pkl'\n",
        "m11 = train_naive_bayes(x_train_1, x_test_1,y_train_1, y_test_1)[0]\n",
        "make_pickl_file(file_11, m11)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73be3bde-8d7b-4507-bf9d-732cc619a6a8",
      "metadata": {
        "id": "73be3bde-8d7b-4507-bf9d-732cc619a6a8"
      },
      "source": [
        "1.2 Stopwords Removed, Bigrams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c0cc6e-7b75-43aa-9e9e-4f47cb750d86",
      "metadata": {
        "id": "f3c0cc6e-7b75-43aa-9e9e-4f47cb750d86"
      },
      "outputs": [],
      "source": [
        "#unigrams \n",
        "x_train_uni_12 = bigram(x_train_1)\n",
        "x_dev_uni_12 = bigram(x_dev_1)\n",
        "x_test_uni_12 = bigram(x_test_1)\n",
        "y_train_uni_12 = bigram(y_train_1)\n",
        "y_dev_uni_12 = bigram(y_dev_1)\n",
        "y_test_uni_12 = bigram(y_test_1)\n",
        "\n",
        "call_for_results(x_train_uni_12, x_test_uni_12,y_train_uni_12, y_test_uni_12)\n",
        "\n",
        "file_12 = 'mnb_bi_ns.pkl'\n",
        "m12 = train_naive_bayes(x_train_uni_12, x_test_uni_12,y_train_uni_12, y_test_uni_12)[0]\n",
        "make_pickl_file(file_12, m12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e17e0d5-fb7d-4412-a5c5-cc3a12e85de3",
      "metadata": {
        "id": "6e17e0d5-fb7d-4412-a5c5-cc3a12e85de3"
      },
      "source": [
        "1.3 Stopwords Removed, unigrams and bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2905ed-8a66-4945-a382-0d3627c30057",
      "metadata": {
        "id": "5e2905ed-8a66-4945-a382-0d3627c30057"
      },
      "outputs": [],
      "source": [
        "x_train_uni_13 = uni_bigram(x_train_1)\n",
        "x_dev_uni_13 = uni_bigram(x_dev_1)\n",
        "x_test_uni_13 = uni_bigram(x_test_1)\n",
        "y_train_uni_13 = uni_bigram(y_train_1)\n",
        "y_dev_uni_13 = uni_bigram(y_dev_1)\n",
        "y_test_uni_13 = uni_bigram(y_test_1)\n",
        "\n",
        "call_for_results(x_train_uni_13, x_test_uni_13,y_train_uni_13, y_test_uni_13)\n",
        "\n",
        "file_13 = 'mnb_uni_bi_ns.pkl'\n",
        "m13 = train_naive_bayes(x_train_uni_13, x_test_uni_13,y_train_uni_13, y_test_uni_13)[0]\n",
        "make_pickl_file(file_13, m13)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb2b90a4-1738-4a27-82fe-cd099b51a4c7",
      "metadata": {
        "id": "eb2b90a4-1738-4a27-82fe-cd099b51a4c7"
      },
      "source": [
        "1.4 Including stopwords, Unigrams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6004e3-572f-4546-8cdf-1b426db1d2c3",
      "metadata": {
        "id": "df6004e3-572f-4546-8cdf-1b426db1d2c3"
      },
      "outputs": [],
      "source": [
        "#unigrams \n",
        "x_train_uni_14 = unigram(x_train)\n",
        "x_dev_uni_14 = unigram(x_dev)\n",
        "x_test_uni_14 = unigram(x_test)\n",
        "y_train_uni_14 = unigram(y_train)\n",
        "y_dev_uni_14 = unigram(y_dev)\n",
        "y_test_uni_14 = unigram(y_test)\n",
        "\n",
        "call_for_results(x_train_uni_14, x_test_uni_14,y_train_uni_14, y_test_uni_14)\n",
        "\n",
        "file_14 = 'mnb_uni.pkl'\n",
        "m14 = train_naive_bayes(x_train_uni_14, x_test_uni_14,y_train_uni_14, y_test_uni_14)[0]\n",
        "make_pickl_file(file_14, m14)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54855a39-0e8b-4291-b07e-bd60bc2b02ec",
      "metadata": {
        "id": "54855a39-0e8b-4291-b07e-bd60bc2b02ec"
      },
      "source": [
        "1.5 Including stopwords, Bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59237459-dfb1-4734-ad62-967b81b75162",
      "metadata": {
        "id": "59237459-dfb1-4734-ad62-967b81b75162"
      },
      "outputs": [],
      "source": [
        "x_train_uni_15 = bigram(x_train)\n",
        "x_dev_uni_15 = bigram(x_dev)\n",
        "x_test_uni_15 = bigram(x_test)\n",
        "y_train_uni_15 = bigram(y_train)\n",
        "y_dev_uni_15 = bigram(y_dev)\n",
        "y_test_uni_15 = bigram(y_test)\n",
        "\n",
        "call_for_results(x_train_uni_15, x_test_uni_15,y_train_uni_15, y_test_uni_15)\n",
        "file_15 = 'mnb_bi.pkl'\n",
        "m15 = train_naive_bayes(x_train_uni_15, x_test_uni_15,y_train_uni_15, y_test_uni_15)[0]\n",
        "make_pickl_file(file_15, m15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4a78a2b-5429-4ece-be90-82d51cc54592",
      "metadata": {
        "id": "d4a78a2b-5429-4ece-be90-82d51cc54592"
      },
      "source": [
        "1.6 Including stopwords, unigrams and bigrams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca554ff6-d8ff-462f-a3d6-2a598fe36c23",
      "metadata": {
        "id": "ca554ff6-d8ff-462f-a3d6-2a598fe36c23"
      },
      "outputs": [],
      "source": [
        "x_train_uni_16 = uni_bigram(x_train)\n",
        "x_dev_uni_16 = uni_bigram(x_dev)\n",
        "x_test_uni_16 = uni_bigram(x_test)\n",
        "y_train_uni_16 = uni_bigram(y_train)\n",
        "y_dev_uni_16 = uni_bigram(y_dev)\n",
        "y_test_uni_16 = uni_bigram(y_test)\n",
        "\n",
        "call_for_results(x_train_uni_16, x_test_uni_16,y_train_uni_16, y_test_uni_16)\n",
        "file_16 = 'mnb_uni_bi.pkl'\n",
        "m16 = train_naive_bayes(x_train_uni_16, x_test_uni_16,y_train_uni_16, y_test_uni_16))[0]\n",
        "make_pickl_file(file_16, m16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c01024d7-5cbe-40fa-89b4-7cb301209e34",
      "metadata": {
        "id": "c01024d7-5cbe-40fa-89b4-7cb301209e34"
      },
      "source": [
        "Task 2. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88bd1cd-699b-448a-9b24-c6c6013dfde8",
      "metadata": {
        "id": "c88bd1cd-699b-448a-9b24-c6c6013dfde8"
      },
      "source": [
        "Question 2.1. \n",
        "\n",
        "Which condition performed better: with or without stopwords? "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af5a7f98-d6ef-4ffd-a623-1cde80868092",
      "metadata": {
        "id": "af5a7f98-d6ef-4ffd-a623-1cde80868092"
      },
      "source": [
        "Question 2.2 \n",
        "\n",
        "Which condition performed better: unigrams, bigrams or unigrams+bigrams?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f558b3ec-88e4-46de-9f6a-81345f74b208",
      "metadata": {
        "id": "f558b3ec-88e4-46de-9f6a-81345f74b208"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "assignment2(2).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}